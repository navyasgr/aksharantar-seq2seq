{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389064c1",
   "metadata": {},
   "source": [
    "# ðŸ”¡ Aksharantar Transliteration â€” LSTM + Bahdanau Attention (GPU-ready)\n",
    "**Colab-ready notebook**: train and evaluate a character-level Seq2Seq transliteration model (Hindi example).\n",
    "This notebook is prepared to run on Google Colab with GPU (recommended). Follow the cells in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1683bb",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "Steps included:\n",
    "1. Verify GPU and install dependencies  \n",
    "2. Create folders and upload dataset (or mount Drive)  \n",
    "3. Define utility functions and dataset class (`utils`)  \n",
    "4. Define model components (`encoder`, `decoder`, `seq2seq`)  \n",
    "5. Training loop (with progress printing)  \n",
    "6. Save checkpoint and run inference  \n",
    "7. Small evaluation (sample predictions)\n",
    "\n",
    "**Dataset expectation**:  \n",
    "Place `hin_train.csv` and `hin_valid.csv` into `data/aksharantar_sampled/hin/`.  \n",
    "The notebook also contains instructions to upload files manually or mount Google Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability and install dependencies (PyTorch is usually preinstalled on Colab)\n",
    "!nvidia-smi || true\n",
    "import torch\n",
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# Install/update required packages (quiet)\n",
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet torch torchvision torchaudio pandas tqdm nbformat\n",
    "print('Dependencies installed (or already present).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43898ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project folders\n",
    "import os\n",
    "os.makedirs('/content/data/aksharantar_sampled/hin', exist_ok=True)\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "os.makedirs('/content/checkpoints', exist_ok=True)\n",
    "print('Folders created under /content')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf700e8",
   "metadata": {},
   "source": [
    "### Upload dataset files\n",
    "Use the left-side **Files** panel in Colab and click **Upload** to upload:\n",
    "- `hin_train.csv`\n",
    "- `hin_valid.csv`\n",
    "\n",
    "Upload them into `/content/data/aksharantar_sampled/hin/`.\n",
    "\n",
    "Alternatively, mount your Google Drive and copy files from Drive:\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp /content/drive/MyDrive/path/to/hin_train.csv /content/data/aksharantar_sampled/hin/\n",
    "!cp /content/drive/MyDrive/path/to/hin_valid.csv /content/data/aksharantar_sampled/hin/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd73f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils: data loading, vocab, dataset, collate\n",
    "import os, pandas as pd, torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_language_pairs(data_dir, lang):\n",
    "    base = os.path.join(data_dir, lang)\n",
    "    train_path = os.path.join(base, f\"{lang}_train.csv\")\n",
    "    valid_path = os.path.join(base, f\"{lang}_valid.csv\")\n",
    "    train_df = pd.read_csv(train_path, header=0)\n",
    "    valid_df = pd.read_csv(valid_path, header=0)\n",
    "    src_col, tgt_col = train_df.columns[:2]\n",
    "    print(f\"âœ… Detected columns: source='{src_col}', target='{tgt_col}'\")\n",
    "    train_pairs = list(zip(train_df[src_col].astype(str), train_df[tgt_col].astype(str)))\n",
    "    valid_pairs = list(zip(valid_df[src_col].astype(str), valid_df[tgt_col].astype(str)))\n",
    "    print(f\"âœ… Loaded {len(train_pairs)} train / {len(valid_pairs)} valid samples\")\n",
    "    return train_pairs, valid_pairs\n",
    "\n",
    "def build_vocab_from_pairs(pairs):\n",
    "    counter = Counter()\n",
    "    for s,t in pairs:\n",
    "        counter.update(list(s))\n",
    "        counter.update(list(t))\n",
    "    tokens = ['<pad>','<sos>','<eos>','<unk>']\n",
    "    stoi = {tok:i for i,tok in enumerate(tokens)}\n",
    "    idx = len(stoi)\n",
    "    for ch in sorted(counter.keys()):\n",
    "        if ch not in stoi:\n",
    "            stoi[ch] = idx\n",
    "            idx += 1\n",
    "    itos = {i:c for c,i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_stoi, tgt_stoi, max_len=30):\n",
    "        self.pairs = pairs\n",
    "        self.src_stoi = src_stoi\n",
    "        self.tgt_stoi = tgt_stoi\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def encode(self, text, stoi):\n",
    "        ids = [stoi.get(ch, stoi['<unk>']) for ch in text][:self.max_len-2]\n",
    "        ids = [stoi['<sos>']] + ids + [stoi['<eos>']]\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [stoi['<pad>']] * (self.max_len - len(ids))\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        s,t = self.pairs[idx]\n",
    "        return self.encode(s, self.src_stoi), self.encode(t, self.tgt_stoi)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    srcs, tgts = zip(*batch)\n",
    "    srcs = torch.stack(srcs)\n",
    "    tgts = torch.stack(tgts)\n",
    "    return srcs, tgts\n",
    "\n",
    "print('utils loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models: encoder, attention, decoder, seq2seq\n",
    "import torch, torch.nn as nn\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.cell_type = cell_type.lower()\n",
    "        if self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(emb)\n",
    "        return outputs, hidden\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "        dec_last = dec_hidden[0][-1].unsqueeze(1) if isinstance(dec_hidden, tuple) else dec_hidden[-1].unsqueeze(1)\n",
    "        score = torch.tanh(self.W1(enc_outputs) + self.W2(dec_last))\n",
    "        score = self.V(score).squeeze(-1)\n",
    "        attn_weights = torch.softmax(score, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, cell_type='lstm', use_attention=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.cell_type = cell_type.lower()\n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            self.attention = BahdanauAttention(hidden_dim)\n",
    "            self.rnn = nn.LSTM(hidden_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, input_tok, hidden, enc_outputs):\n",
    "        emb = self.embedding(input_tok).unsqueeze(1)\n",
    "        if self.use_attention:\n",
    "            h = hidden[0] if isinstance(hidden, tuple) else hidden\n",
    "            context, attn = self.attention(h, enc_outputs)\n",
    "            rnn_input = torch.cat((emb, context.unsqueeze(1)), dim=2)\n",
    "        else:\n",
    "            rnn_input = emb\n",
    "        out, hidden = self.rnn(rnn_input, hidden)\n",
    "        pred = self.fc(out.squeeze(1))\n",
    "        return pred, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch, seq = src.size()\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch, tgt_len, vocab_size).to(self.device)\n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        input_tok = tgt[:,0]\n",
    "        for t in range(1, tgt_len):\n",
    "            pred, hidden = self.decoder(input_tok, hidden, enc_outputs)\n",
    "            outputs[:,t,:] = pred\n",
    "            top1 = pred.argmax(1)\n",
    "            input_tok = tgt[:,t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        return outputs\n",
    "\n",
    "print('models loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a458cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell (adjust hyperparams as needed)\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "DATA_DIR = '/content/data/aksharantar_sampled'\n",
    "LANG = 'hin'\n",
    "MAX_LEN = 30\n",
    "BATCH = 128\n",
    "HID = 256\n",
    "EPOCHS = 5  # increase to 25+ for final training\n",
    "\n",
    "train_pairs, valid_pairs = load_language_pairs(DATA_DIR, LANG)\n",
    "pairs = train_pairs + valid_pairs\n",
    "src_stoi, src_itos = build_vocab_from_pairs(pairs)\n",
    "tgt_stoi, tgt_itos = build_vocab_from_pairs(pairs)\n",
    "\n",
    "train_ds = TransliterationDataset(train_pairs, src_stoi, tgt_stoi, max_len=MAX_LEN)\n",
    "valid_ds = TransliterationDataset(valid_pairs, src_stoi, tgt_stoi, max_len=MAX_LEN)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "enc = EncoderRNN(len(src_stoi), HID, cell_type='lstm').to(device)\n",
    "dec = DecoderRNN(len(tgt_stoi), HID, cell_type='lstm', use_attention=True).to(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_stoi['<pad>'])\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt)\n",
    "        loss = criterion(out[:,1:].reshape(-1, out.shape[-1]), tgt[:,1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch}/{EPOCHS} â€” Train Loss: {avg:.4f}')\n",
    "    # quick validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in valid_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            out = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            val_loss += criterion(out[:,1:].reshape(-1, out.shape[-1]), tgt[:,1:].reshape(-1)).item()\n",
    "    print(f'Val Loss: {val_loss / len(valid_loader):.4f}')\n",
    "    torch.save(model.state_dict(), f'/content/checkpoints/{LANG}_epoch{epoch}.pt')\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference example (after training)\n",
    "# Build reverse mappings\n",
    "src_itos = {i:c for c,i in src_stoi.items()}\n",
    "tgt_itos = {i:c for c,i in tgt_stoi.items()}\n",
    "\n",
    "def predict_word(word, max_len=30):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = [src_stoi.get(ch, src_stoi['<unk>']) for ch in word][:max_len-2]\n",
    "        src_tensor = torch.tensor([ [src_stoi['<sos>']] + ids + [src_stoi['<eos>']] + [src_stoi['<pad>']]*(max_len - (len(ids)+2)) ], dtype=torch.long).to(device)\n",
    "        enc_out, hidden = enc(src_tensor)\n",
    "        input_tok = torch.tensor([tgt_stoi['<sos>']]).to(device)\n",
    "        out_s = ''\n",
    "        for _ in range(max_len):\n",
    "            pred, hidden = dec(input_tok, hidden, enc_out)\n",
    "            tok = pred.argmax(1).item()\n",
    "            if tok == tgt_stoi.get('<eos>', -1):\n",
    "                break\n",
    "            out_s += tgt_itos.get(tok, '')\n",
    "            input_tok = torch.tensor([tok]).to(device)\n",
    "    return out_s\n",
    "\n",
    "# Try example (after training)\n",
    "print('Sample:', predict_word('bindhya'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final notebook model to Drive if desired (mount first)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp /content/checkpoints/hin_epoch25.pt /content/drive/MyDrive/\n",
    "print('Use Drive copy commands to save checkpoints to your Drive')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
